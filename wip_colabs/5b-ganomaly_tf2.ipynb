{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manmeet3/Masters_Project/blob/master/wip_colabs/5b-ganomaly_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXkoVEkWronw"
      },
      "source": [
        "# https://www.groundai.com/project/ganomaly-semi-supervised-anomaly-detection-via-adversarial-training/1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGwuLlM7rs7o"
      },
      "source": [
        "https://github.com/chychen/tf2-ganomaly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1LytNWdqguQ",
        "outputId": "27cd8134-2f56-4fcb-b904-b339d51d3747"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4odjevGqhkd",
        "outputId": "9a21b07f-8b88-464c-fa30-52d5666a3cf5"
      },
      "source": [
        "%cd /content/drive/MyDrive/Masters_Project/Datasets/OASIS3/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Masters_Project/Datasets/OASIS3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFf-uzo_q7iL"
      },
      "source": [
        "# %cd ganomaly_tf2\n",
        "# !wget https://raw.githubusercontent.com/chychen/tf2-ganomaly/master/metrics.py\n",
        "# !wget https://raw.githubusercontent.com/chychen/tf2-ganomaly/master/model.py\n",
        "# !wget https://raw.githubusercontent.com/chychen/tf2-ganomaly/master/train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AATRtv9srdvp",
        "outputId": "a88b2423-2192-4ecb-a67b-e2807c3d1ef8"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "print(tf.__version__)\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from ganomaly_tf2.model import GANomaly\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)\n",
        "logging.set_stderrthreshold(logging.INFO)\n",
        "\n",
        "from typing import Any, Callable, Optional, Tuple\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueHQqtypr8kH"
      },
      "source": [
        "def batch_resize(imgs, size: tuple):\n",
        "    img_out = np.empty((imgs.shape[0],) + size)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        img_out[i] = cv2.resize(imgs[i], size, interpolation=cv2.INTER_CUBIC)\n",
        "    return img_out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjZwaE0KsCyO"
      },
      "source": [
        "class Option:\n",
        "    def __init__(self):\n",
        "        self.anomaly = 1 # the anomaly digit\n",
        "        self.shuffle_buffer_size = 10000\n",
        "        self.batch_size = 8\n",
        "        self.isize = 256 # input size\n",
        "        self.ckpt_dir = \"ckpt\"\n",
        "        self.nz = 100 # latent dims\n",
        "        self.nc = 1 # input channels\n",
        "        self.ndf = 64 # number of discriminator's filters\n",
        "        self.ngf = 64 # number of generator's filters\n",
        "        self.extralayers = 0\n",
        "        self.niter = 15 # number of training epochs\n",
        "        self.lr = 2e-4 \n",
        "        self.w_adv = 1. # Adversarial loss weight\n",
        "        self.w_con = 50. # Reconstruction loss weight\n",
        "        self.w_enc = 1. # Encoder loss weight.\n",
        "        self.beta1 = 0.5\n",
        "        self.encdims = None\n",
        "            \n",
        "opt = Option()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkAMiBZKytN2"
      },
      "source": [
        "#data_train, data_test = tf.keras.datasets.mnist.load_data()\n",
        "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otGVmqKZ72_o"
      },
      "source": [
        "def read_data(path, normal=True):\n",
        "    data: Any = []\n",
        "    labels = []\n",
        "    \n",
        "    for file_path in glob.glob(path):\n",
        "        with open (file_path, 'rb') as f:\n",
        "            # image needs to be a PIL image\n",
        "            img = Image.open(f)\n",
        "            # Resize all images 176, 256, 3 -> 256, 256, 0\n",
        "            dsize = (256, 256)\n",
        "            resized = img.resize(dsize)\n",
        "            data.append(resized)\n",
        "            labels.append(1 if normal else 0) \n",
        "    data = np.vstack(data).reshape(-1, 256, 256)\n",
        "    data = data.transpose((0, 1, 2))  # convert to HWC\n",
        "    return (data, labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwounWgO1nR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f759af60-97f8-46f1-91ce-705908150978"
      },
      "source": [
        "train_image_number = 140 # Image slice from MRI scans to use for training data\n",
        "#root_dir = '/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/'\n",
        "root = '/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/'\n",
        "non_ad_jpg_folder = 't1w_non_ad_jpgs/' + str(train_image_number) + '/'\n",
        "ad_jpg_folder = 't1w_ad_jpgs/' + str(train_image_number) + '/'\n",
        "\n",
        "\n",
        "ad_jpg_folder = os.path.join(root, ad_jpg_folder)\n",
        "non_ad_jpg_folder = os.path.join(root, non_ad_jpg_folder)\n",
        "    \n",
        "print(\"=\"*5 + \" Loading Data \" + \"=\"*5)\n",
        "print(f\"ad_jpg_folder: {ad_jpg_folder}\")\n",
        "print(f\"non_ad_jpg_folder: {non_ad_jpg_folder}\")\n",
        "\n",
        "non_ad_data: Any = []\n",
        "non_ad_targets = []\n",
        "\n",
        "ad_data: Any = []\n",
        "ad_targets = []\n",
        "\n",
        "# Read data and label into a numpy array\n",
        "non_ad_data, non_ad_targets = read_data(ad_jpg_folder+'**.jpg', normal=True)\n",
        "ad_data, ad_targets = read_data(non_ad_jpg_folder+'**.jpg', normal=False)\n",
        "\n",
        "# shuffle the datasets and bucket them as test and train\n",
        "# (non_ad_data, non_ad_targets) = shuffle_associated_arrays(non_ad_data, non_ad_targets)\n",
        "# (ad_data, ad_targets) = shuffle_associated_arrays(ad_data, ad_targets)\n",
        "\n",
        "all_data = np.concatenate((non_ad_data, ad_data), axis=0)\n",
        "all_targets = non_ad_targets + ad_targets\n",
        "\n",
        "x_train, x_test, y_train, y_test =\\\n",
        "  train_test_split(all_data, all_targets, test_size=0.1, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Loading Data =====\n",
            "ad_jpg_folder: /content/drive/MyDrive/Masters_Project/Datasets/OASIS3/t1w_ad_jpgs/140/\n",
            "non_ad_jpg_folder: /content/drive/MyDrive/Masters_Project/Datasets/OASIS3/t1w_non_ad_jpgs/140/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jilyDAnIDad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef70d51-4647-4de7-bc6a-3f2a094cf04e"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1165, 256, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-x5XcnOW7t_"
      },
      "source": [
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "\n",
        "y_train = np.array(y_train, dtype=np.intc)\n",
        "y_test = np.array(y_test, dtype=np.intc)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnuODPLIXho3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f51268-451b-4f6e-a2db-c70bfe94fcdc"
      },
      "source": [
        "x_train = batch_resize(x_train, (256, 256))[..., None]\n",
        "x_test = batch_resize(x_test, (256, 256))[..., None]\n",
        "# normalization\n",
        "mean = x_train.mean()\n",
        "stddev = x_train.std()\n",
        "x_train = (x_train-mean)/stddev\n",
        "x_test = (x_test-mean)/stddev\n",
        "print(x_train.shape, x_train.shape)\n",
        "# define abnoraml data and normal\n",
        "# training data only contains normal\n",
        "x_train = x_train[y_train!=opt.anomaly]\n",
        "y_train = y_train[y_train!=opt.anomaly]\n",
        "y_test = (y_test==opt.anomaly).astype(np.float32)\n",
        "# tf.data.Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_dataset = train_dataset.shuffle(opt.shuffle_buffer_size).batch(opt.batch_size, drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(opt.batch_size, drop_remainder=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1165, 256, 256, 1) (1165, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW7nfXrVYKE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7cbb71-de28-41bc-8c84-7c57c4221c86"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((8, 256, 256, 1), (8,)), types: (tf.float64, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y5E8hGtYzr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7a5ea4-7a29-4001-fc2c-b431ac36186b"
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 256, 256, 1), (None,)), types: (tf.float64, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MknPJHZw1_E",
        "outputId": "b57c015a-9cb5-4ea1-c077-2d030cbb8ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dir(test_dataset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_GeneratorState',\n",
              " '__abstractmethods__',\n",
              " '__bool__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__nonzero__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_add_variable_with_custom_getter',\n",
              " '_apply_options',\n",
              " '_as_serialized_graph',\n",
              " '_batch_size',\n",
              " '_checkpoint_dependencies',\n",
              " '_consumers',\n",
              " '_deferred_dependencies',\n",
              " '_drop_remainder',\n",
              " '_flat_shapes',\n",
              " '_flat_structure',\n",
              " '_flat_types',\n",
              " '_functions',\n",
              " '_gather_saveables_for_checkpoint',\n",
              " '_graph',\n",
              " '_graph_attr',\n",
              " '_handle_deferred_dependencies',\n",
              " '_has_captured_ref',\n",
              " '_input_dataset',\n",
              " '_inputs',\n",
              " '_list_extra_dependencies_for_serialization',\n",
              " '_list_functions_for_serialization',\n",
              " '_lookup_dependency',\n",
              " '_map_resources',\n",
              " '_maybe_initialize_trackable',\n",
              " '_name_based_attribute_restore',\n",
              " '_name_based_restores',\n",
              " '_no_dependency',\n",
              " '_object_identifier',\n",
              " '_options_attr',\n",
              " '_preload_simple_restoration',\n",
              " '_restore_from_checkpoint_position',\n",
              " '_self_name_based_restores',\n",
              " '_self_saveable_object_factories',\n",
              " '_self_setattr_tracking',\n",
              " '_self_unconditional_checkpoint_dependencies',\n",
              " '_self_unconditional_deferred_dependencies',\n",
              " '_self_unconditional_dependency_names',\n",
              " '_self_update_uid',\n",
              " '_setattr_tracking',\n",
              " '_shape_invariant_to_type_spec',\n",
              " '_single_restoration_from_checkpoint_position',\n",
              " '_structure',\n",
              " '_tf_api_names',\n",
              " '_tf_api_names_v1',\n",
              " '_trace_variant_creation',\n",
              " '_track_trackable',\n",
              " '_tracking_metadata',\n",
              " '_type_spec',\n",
              " '_unconditional_checkpoint_dependencies',\n",
              " '_unconditional_dependency_names',\n",
              " '_update_uid',\n",
              " '_variant_tensor',\n",
              " '_variant_tensor_attr',\n",
              " '_variant_tracker',\n",
              " 'apply',\n",
              " 'as_numpy_iterator',\n",
              " 'batch',\n",
              " 'cache',\n",
              " 'cardinality',\n",
              " 'concatenate',\n",
              " 'element_spec',\n",
              " 'enumerate',\n",
              " 'filter',\n",
              " 'flat_map',\n",
              " 'from_generator',\n",
              " 'from_tensor_slices',\n",
              " 'from_tensors',\n",
              " 'interleave',\n",
              " 'list_files',\n",
              " 'map',\n",
              " 'options',\n",
              " 'padded_batch',\n",
              " 'prefetch',\n",
              " 'range',\n",
              " 'reduce',\n",
              " 'repeat',\n",
              " 'shard',\n",
              " 'shuffle',\n",
              " 'skip',\n",
              " 'take',\n",
              " 'unbatch',\n",
              " 'window',\n",
              " 'with_options',\n",
              " 'zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW1GpM5-Y3St"
      },
      "source": [
        "ganomaly = GANomaly(opt, train_dataset, valid_dataset=None, test_dataset=test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbKwFAYCY9Zc"
      },
      "source": [
        "ganomaly.fit(opt.niter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4XxkWJ_aY76"
      },
      "source": [
        "ganomaly.evaluate_best(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZc1lU5WbaE5"
      },
      "source": [
        "dir(ganomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9OUU-lcb71Q"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXpLdgDT_st1"
      },
      "source": [
        "#ganomaly.save(\"./ganomaly_tf2/output_latest/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orxpg9dxtd6z"
      },
      "source": [
        "D = tf.keras.models.load_model('save_model/disc', compile=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPUWCEfHtzeg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}