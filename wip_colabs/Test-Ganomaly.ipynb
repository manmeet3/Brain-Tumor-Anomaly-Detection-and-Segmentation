{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Ganomaly.ipynb  data  ganomaly\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ganomaly' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/samet-akcay/ganomaly.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: conda: not found\r\n"
     ]
    }
   ],
   "source": [
    "!conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asn1crypto==0.24.0 in /usr/lib/python3/dist-packages (from -r ganomaly/requirements.txt (line 1)) (0.24.0)\n",
      "Collecting certifi==2019.6.16\n",
      "  Downloading certifi-2019.6.16-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cffi==1.12.3\n",
      "  Downloading cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl (430 kB)\n",
      "\u001b[K     |████████████████████████████████| 430 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r ganomaly/requirements.txt (line 4)) (3.0.4)\n",
      "Collecting cryptography==2.7\n",
      "  Downloading cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r ganomaly/requirements.txt (line 6)) (0.10.0)\n",
      "Collecting idna==2.8\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib==0.13.2\n",
      "  Downloading joblib-0.13.2-py2.py3-none-any.whl (278 kB)\n",
      "\u001b[K     |████████████████████████████████| 278 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver==1.1.0\n",
      "  Downloading kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 8.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.1.0\n",
      "  Downloading matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.16.4\n",
      "  Downloading numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting olefile==0.46\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.6/dist-packages (from -r ganomaly/requirements.txt (line 13)) (8.0.1)\n",
      "Collecting pycparser==2.19\n",
      "  Downloading pycparser-2.19.tar.gz (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyOpenSSL==19.0.0\n",
      "  Downloading pyOpenSSL-19.0.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing==2.4.0\n",
      "  Downloading pyparsing-2.4.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting PySocks==1.7.0\n",
      "  Downloading PySocks-1.7.0-py3-none-any.whl (16 kB)\n",
      "Collecting python-dateutil==2.8.0\n",
      "  Downloading python_dateutil-2.8.0-py2.py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz==2019.1\n",
      "  Downloading pytz-2019.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyzmq==18.0.2\n",
      "  Downloading pyzmq-18.0.2-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.22.0\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.21.2\n",
      "  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.3.0\n",
      "  Downloading scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.2 MB 6.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting six==1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting torch==1.2.0\n",
      "  Downloading torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 26 kB/s  eta 0:00:013     |█████▋                          | 132.0 MB 9.5 MB/s eta 0:01:06     |█████████████████▊              | 415.1 MB 11.6 MB/s eta 0:00:29     |████████████████████▋           | 481.6 MB 12.6 MB/s eta 0:00:22     |█████████████████████           | 492.2 MB 7.3 MB/s eta 0:00:36\n",
      "\u001b[?25hCollecting torchfile==0.1.0\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "Collecting torchvision==0.4\n",
      "  Downloading torchvision-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 4.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tornado==6.0.3\n",
      "  Downloading tornado-6.0.3.tar.gz (482 kB)\n",
      "\u001b[K     |████████████████████████████████| 482 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.33.0\n",
      "  Downloading tqdm-4.33.0-py2.py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3==1.25.3\n",
      "  Downloading urllib3-1.25.3-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[K     |████████████████████████████████| 150 kB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting visdom==0.1.8.8\n",
      "  Downloading visdom-0.1.8.8.tar.gz (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting websocket-client==0.56.0\n",
      "  Downloading websocket_client-0.56.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver==1.1.0->-r ganomaly/requirements.txt (line 9)) (51.0.0)\n",
      "Building wheels for collected packages: olefile, pycparser, torchfile, tornado, visdom\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=cc52758c21834ca9bdef0450880d7b263b8a81d70c79d0ee4364d98c708178cc\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/9d/f9/4f5e50f497a2ed398d9786244df90cee57583e92bff76bfc2b\n",
      "  Building wheel for pycparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111030 sha256=7e0bd39053f9ace2b58b70582570be730474c86e320ee36bdd4c9d1f488aad4f\n",
      "  Stored in directory: /root/.cache/pip/wheels/c6/6b/83/2608afaa57ecfb0a66ac89191a8d9bad71c62ca55ee499c2d0\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5712 sha256=56d046c8ea362e979cce35d72f85e2638a328e50dd0be7d30f275a2c4f0c8f1b\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/79/ec/084a3a2e348d72852cc0c13c559c923c13ca54db86e699b681\n",
      "  Building wheel for tornado (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tornado: filename=tornado-6.0.3-cp36-cp36m-linux_x86_64.whl size=423187 sha256=f1cf63e003d85fbac2d52b07e10151ee6d6986be00eebe372ee3aaf1311c4af4\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/92/5c/3bfc125cdf46ab0487727e574f513f9568e9b7974b15237abf\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visdom: filename=visdom-0.1.8.8-py3-none-any.whl size=1350602 sha256=48fb8b4e25c6dfc5e3f50e1650ac99460eafb8b6cfcee6c4896e36f103f58310\n",
      "  Stored in directory: /root/.cache/pip/wheels/d7/a7/f0/f674fe15e667c8344f2c8ce67581467cd35a77a17620daa475\n",
      "Successfully built olefile pycparser torchfile tornado visdom\n",
      "Installing collected packages: certifi, pycparser, cffi, six, cryptography, idna, joblib, kiwisolver, python-dateutil, pyparsing, numpy, matplotlib, olefile, pyOpenSSL, PySocks, pytz, pyzmq, urllib3, requests, scipy, scikit-learn, torch, torchfile, torchvision, tornado, tqdm, websocket-client, visdom\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.12.5\n",
      "    Uninstalling certifi-2020.12.5:\n",
      "      Successfully uninstalled certifi-2020.12.5\n",
      "  Attempting uninstall: pycparser\n",
      "    Found existing installation: pycparser 2.20\n",
      "    Uninstalling pycparser-2.20:\n",
      "      Successfully uninstalled pycparser-2.20\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.14.4\n",
      "    Uninstalling cffi-1.14.4:\n",
      "      Successfully uninstalled cffi-1.14.4\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 2.1.4\n",
      "    Uninstalling cryptography-2.1.4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled cryptography-2.1.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.6\n",
      "    Uninstalling idna-2.6:\n",
      "      Successfully uninstalled idna-2.6\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.3.1\n",
      "    Uninstalling kiwisolver-1.3.1:\n",
      "      Successfully uninstalled kiwisolver-1.3.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.3\n",
      "    Uninstalling matplotlib-3.3.3:\n",
      "      Successfully uninstalled matplotlib-3.3.3\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 20.0.0\n",
      "    Uninstalling pyzmq-20.0.0:\n",
      "      Successfully uninstalled pyzmq-20.0.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.2\n",
      "    Uninstalling urllib3-1.26.2:\n",
      "      Successfully uninstalled urllib3-1.26.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.0\n",
      "    Uninstalling requests-2.25.0:\n",
      "      Successfully uninstalled requests-2.25.0\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.1\n",
      "    Uninstalling tornado-6.1:\n",
      "      Successfully uninstalled tornado-6.1\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tf-nightly-gpu 2.5.0.dev20201210 requires numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\n",
      "tf-nightly-gpu 2.5.0.dev20201210 requires six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\n",
      "nbclient 0.5.1 requires nbformat>=5.0, but you'll have nbformat 4.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed PySocks-1.7.0 certifi-2019.6.16 cffi-1.12.3 cryptography-2.7 idna-2.8 joblib-0.13.2 kiwisolver-1.1.0 matplotlib-3.1.0 numpy-1.16.4 olefile-0.46 pyOpenSSL-19.0.0 pycparser-2.19 pyparsing-2.4.0 python-dateutil-2.8.0 pytz-2019.1 pyzmq-18.0.2 requests-2.22.0 scikit-learn-0.21.2 scipy-1.3.0 six-1.12.0 torch-1.2.0 torchfile-0.1.0 torchvision-0.4.0 tornado-6.0.3 tqdm-4.33.0 urllib3-1.25.3 visdom-0.1.8.8 websocket-client-0.56.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ganomaly/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mkl-fft\n",
      "  Downloading mkl_fft-1.0.6-cp36-cp36m-manylinux1_x86_64.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting intel-numpy\n",
      "  Downloading intel_numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tbb4py\n",
      "  Downloading tbb4py-2021.1.1-py36-none-manylinux1_x86_64.whl (425 kB)\n",
      "\u001b[K     |████████████████████████████████| 425 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mkl\n",
      "  Downloading mkl-2021.1.1-py2.py3-none-manylinux1_x86_64.whl (737.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 737.3 MB 5.3 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting icc-rt\n",
      "  Downloading icc_rt-2020.0.133-py2.py3-none-manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mkl-random\n",
      "  Downloading mkl_random-1.0.1.1-cp36-cp36m-manylinux1_x86_64.whl (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tbb==2021.1.1\n",
      "  Downloading tbb-2021.1.1-py2.py3-none-manylinux1_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting intel-openmp==2021.*\n",
      "  Downloading intel_openmp-2021.1.2-py2.py3-none-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tbb, tbb4py, intel-openmp, mkl, icc-rt, mkl-random, intel-numpy, mkl-fft\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "icc-rt 2020.0.133 requires intel-openmp==2020.*, but you'll have intel-openmp 2021.1.2 which is incompatible.\u001b[0m\n",
      "Successfully installed icc-rt-2020.0.133 intel-numpy-1.15.1 intel-openmp-2021.1.2 mkl-2021.1.1 mkl-fft-1.0.6 mkl-random-1.0.1.1 tbb-2021.1.1 tbb4py-2021.1.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mkl-fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "OPT_isize = 32\n",
    "OPT_abnormal_class = \"plane\"\n",
    "OPT_manualseed = 55\n",
    "OPT_batchsize = 64\n",
    "OPT_workers = 8\n",
    "\n",
    "splits = ['train', 'test']\n",
    "drop_last_batch = {'train': True, 'test': False}\n",
    "shuffle = {'train': True, 'test': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(OPT_isize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classes = {\n",
    "    'plane': 0, 'car': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
    "    'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "dataset['train'] = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataset['test'] = CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "def get_cifar_anomaly_dataset(trn_img, trn_lbl, tst_img, tst_lbl, abn_cls_idx=0, manualseed=-1):\n",
    "    \"\"\"[summary]\n",
    "    Arguments:\n",
    "        trn_img {np.array} -- Training images\n",
    "        trn_lbl {np.array} -- Training labels\n",
    "        tst_img {np.array} -- Test     images\n",
    "        tst_lbl {np.array} -- Test     labels\n",
    "    Keyword Arguments:\n",
    "        abn_cls_idx {int} -- Anomalous class index (default: {0})\n",
    "    Returns:\n",
    "        [np.array] -- New training-test images and labels.\n",
    "    \"\"\"\n",
    "    # Convert train-test labels into numpy array.\n",
    "    trn_lbl = np.array(trn_lbl)\n",
    "    tst_lbl = np.array(tst_lbl)\n",
    "\n",
    "    # --\n",
    "    # Find idx, img, lbl for abnormal and normal on org dataset.\n",
    "    nrm_trn_idx = np.where(trn_lbl != abn_cls_idx)[0]\n",
    "    abn_trn_idx = np.where(trn_lbl == abn_cls_idx)[0]\n",
    "    nrm_trn_img = trn_img[nrm_trn_idx]    # Normal training images\n",
    "    abn_trn_img = trn_img[abn_trn_idx]    # Abnormal training images\n",
    "    nrm_trn_lbl = trn_lbl[nrm_trn_idx]    # Normal training labels\n",
    "    abn_trn_lbl = trn_lbl[abn_trn_idx]    # Abnormal training labels.\n",
    "\n",
    "    nrm_tst_idx = np.where(tst_lbl != abn_cls_idx)[0]\n",
    "    abn_tst_idx = np.where(tst_lbl == abn_cls_idx)[0]\n",
    "    nrm_tst_img = tst_img[nrm_tst_idx]    # Normal training images\n",
    "    abn_tst_img = tst_img[abn_tst_idx]    # Abnormal training images.\n",
    "    nrm_tst_lbl = tst_lbl[nrm_tst_idx]    # Normal training labels\n",
    "    abn_tst_lbl = tst_lbl[abn_tst_idx]    # Abnormal training labels.\n",
    "\n",
    "    # --\n",
    "    # Assign labels to normal (0) and abnormals (1)\n",
    "    nrm_trn_lbl[:] = 0\n",
    "    nrm_tst_lbl[:] = 0\n",
    "    abn_trn_lbl[:] = 1\n",
    "    abn_tst_lbl[:] = 1\n",
    "\n",
    "    # --\n",
    "    if manualseed != -1:\n",
    "        # Random seed.\n",
    "        # Concatenate the original train and test sets.\n",
    "        nrm_img = np.concatenate((nrm_trn_img, nrm_tst_img), axis=0)\n",
    "        nrm_lbl = np.concatenate((nrm_trn_lbl, nrm_tst_lbl), axis=0)\n",
    "        abn_img = np.concatenate((abn_trn_img, abn_tst_img), axis=0)\n",
    "        abn_lbl = np.concatenate((abn_trn_lbl, abn_tst_lbl), axis=0)\n",
    "\n",
    "        # Split the normal data into the new train and tests.\n",
    "        idx = np.arange(len(nrm_lbl))\n",
    "        np.random.seed(manualseed)\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        nrm_trn_len = int(len(idx) * 0.80)\n",
    "        nrm_trn_idx = idx[:nrm_trn_len]\n",
    "        nrm_tst_idx = idx[nrm_trn_len:]\n",
    "\n",
    "        nrm_trn_img = nrm_img[nrm_trn_idx]\n",
    "        nrm_trn_lbl = nrm_lbl[nrm_trn_idx]\n",
    "        nrm_tst_img = nrm_img[nrm_tst_idx]\n",
    "        nrm_tst_lbl = nrm_lbl[nrm_tst_idx]\n",
    "\n",
    "    # Create new anomaly dataset based on the following data structure:\n",
    "    # - anomaly dataset\n",
    "    #   . -> train\n",
    "    #        . -> normal\n",
    "    #   . -> test\n",
    "    #        . -> normal\n",
    "    #        . -> abnormal\n",
    "    new_trn_img = np.copy(nrm_trn_img)\n",
    "    new_trn_lbl = np.copy(nrm_trn_lbl)\n",
    "    new_tst_img = np.concatenate((nrm_tst_img, abn_trn_img, abn_tst_img), axis=0)\n",
    "    new_tst_lbl = np.concatenate((nrm_tst_lbl, abn_trn_lbl, abn_tst_lbl), axis=0)\n",
    "\n",
    "    return new_trn_img, new_trn_lbl, new_tst_img, new_tst_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].data, dataset['train'].targets, \\\n",
    "dataset['test'].data, dataset['test'].targets = get_cifar_anomaly_dataset(\n",
    "    trn_img=dataset['train'].data,\n",
    "    trn_lbl=dataset['train'].targets,\n",
    "    tst_img=dataset['test'].data,\n",
    "    tst_lbl=dataset['test'].targets,\n",
    "    abn_cls_idx=classes[OPT_abnormal_class],\n",
    "    manualseed=OPT_manualseed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {x: torch.utils.data.DataLoader(dataset=dataset[x],\n",
    "                                             batch_size=OPT_batchsize,\n",
    "                                             shuffle=shuffle[x],\n",
    "                                             num_workers=int(OPT_workers),\n",
    "                                             drop_last=drop_last_batch[x],\n",
    "                                             worker_init_fn=(None if OPT_manualseed == -1\n",
    "                                             else lambda x: np.random.seed(OPT_manualseed)))\n",
    "              for x in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_index_sampler',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'dataset_kind',\n",
       " 'drop_last',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'pin_memory',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataloader['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(dataloader))\n",
    "type(dataloader['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through and print few of the data types stored within the iterator to examine it\n",
    "# Create a similar iterator for the OASIS-3 no-ad scans directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
