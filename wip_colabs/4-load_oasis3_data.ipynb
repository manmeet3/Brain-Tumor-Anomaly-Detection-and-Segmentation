{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-load_oasis3_data.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvU5vMeB3tL8",
        "outputId": "8d7a4d62-020b-4c34-f0cf-295ce9717d35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPD5t7AP35Ba",
        "outputId": "3a5f1c55-d74e-458c-c400-ea091aa0973d"
      },
      "source": [
        "%cd /content/drive/MyDrive/Masters_Project/Datasets/OASIS3/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1bqqm9-0FMOqiqyH_NIQBh-lJ36pWa7aX/Masters_Project/Datasets/OASIS3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZu16iC63UUK",
        "outputId": "12afa7cb-8a40-46bc-fb20-c53e185ddf28"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cifar-10-batches-py\t  fa20\t\t  t1w_non_ad_jpgs\n",
            " cifar-10-python.tar.gz   ganomaly\t  t1w_non_ad_patients\n",
            " data\t\t\t  oasis-scripts  'Usage Instructions.gdoc'\n",
            " downloaded_data\t  t1w_ad_jpgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dXxoGdws3UUV",
        "outputId": "64a750b6-cdb8-43c7-a869-e1d489f70cd8"
      },
      "source": [
        "!pip install -r ../../Code/ganomaly/requirements.txt\r\n",
        "#!pip install -r ./ganomaly/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: asn1crypto==0.24.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 1)) (0.24.0)\n",
            "Requirement already satisfied: certifi==2019.6.16 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 2)) (2019.6.16)\n",
            "Requirement already satisfied: cffi==1.12.3 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 3)) (1.12.3)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: cryptography==2.7 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 5)) (2.7)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: idna==2.8 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 7)) (2.8)\n",
            "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 8)) (0.13.2)\n",
            "Requirement already satisfied: kiwisolver==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: matplotlib==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 10)) (3.1.0)\n",
            "Collecting numpy==1.16.4\n",
            "  Using cached https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile==0.46 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 12)) (0.46)\n",
            "Collecting Pillow>=7.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/eb/8e/d2f7a67cf8da9b83c1e3ee38dbf49448f3c8acb2cb38f76e4301f4a70223/Pillow-8.1.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: pycparser==2.19 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 14)) (2.19)\n",
            "Requirement already satisfied: pyOpenSSL==19.0.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 15)) (19.0.0)\n",
            "Requirement already satisfied: pyparsing==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 16)) (2.4.0)\n",
            "Requirement already satisfied: PySocks==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 17)) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 18)) (2.8.0)\n",
            "Requirement already satisfied: pytz==2019.1 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 19)) (2019.1)\n",
            "Requirement already satisfied: pyzmq==18.0.2 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 20)) (18.0.2)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 21)) (2.22.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 22)) (0.21.2)\n",
            "Requirement already satisfied: scipy==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 23)) (1.3.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 24)) (1.12.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 25)) (1.2.0)\n",
            "Requirement already satisfied: torchfile==0.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 26)) (0.1.0)\n",
            "Requirement already satisfied: torchvision==0.4 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 27)) (0.4.0)\n",
            "Requirement already satisfied: tornado==6.0.3 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 28)) (6.0.3)\n",
            "Requirement already satisfied: tqdm==4.33.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 29)) (4.33.0)\n",
            "Requirement already satisfied: urllib3==1.25.3 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 30)) (1.25.3)\n",
            "Requirement already satisfied: visdom==0.1.8.8 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 31)) (0.1.8.8)\n",
            "Requirement already satisfied: websocket-client==0.56.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 32)) (0.56.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from kiwisolver==1.1.0->-r ../../Code/ganomaly/requirements.txt (line 9)) (53.0.0)\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.21.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mkl-fft 1.2.0 has requirement numpy==1.19.5, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement joblib>=0.14, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, Pillow\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: Pillow 6.2.2\n",
            "    Uninstalling Pillow-6.2.2:\n",
            "      Successfully uninstalled Pillow-6.2.2\n",
            "Successfully installed Pillow-8.1.0 numpy-1.16.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "LJXouR6y3UUW",
        "outputId": "662d4b92-3854-4b76-aae6-f851a7e095f9"
      },
      "source": [
        "!pip install mkl-fft"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: dpcpp_cpp_rt in /usr/local/lib/python3.7/dist-packages (from mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from mkl-fft) (2019.0)\n",
            "Collecting numpy==1.19.5\n",
            "  Using cached https://files.pythonhosted.org/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: common-cmplr-lib-rt==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: common-cmplr-lic-rt==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: intel-openmp==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: opencl-rt==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: tbb==2021.* in /usr/local/lib/python3.7/dist-packages (from opencl-rt==2021.*->dpcpp_cpp_rt->mkl-fft) (2021.1.1)\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.21.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement joblib>=0.14, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed numpy-1.19.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "ooHk9Hpm5aww",
        "outputId": "6bcbbfec-f90a-45b1-e57f-8675b9813284"
      },
      "source": [
        "!pip install Pillow==6.2.2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==6.2.2\n",
            "  Using cached https://files.pythonhosted.org/packages/c3/3f/03375124676ab49ca6e6917c0f1f663afb8354d5d24e12f4fe4587a39ae2/Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 8.1.0\n",
            "    Uninstalling Pillow-8.1.0:\n",
            "      Successfully uninstalled Pillow-8.1.0\n",
            "Successfully installed Pillow-6.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWYyHTRj9Pi6"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPv2uxvO3UUW"
      },
      "source": [
        "import os\n",
        "#import os.path\n",
        "#import cv2\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "from typing import Any, Callable, Optional, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.datasets.utils import check_integrity, download_and_extract_archive"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxCZS52C3UUW",
        "outputId": "cbfea61f-2947-4b02-dfb3-d5122d9f81e3"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oJArWMRm3UUX",
        "outputId": "ac8d2d6a-6362-47f8-bc87-9bddf4883df8"
      },
      "source": [
        "import torchvision\n",
        "torchvision.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTPtCs-A9RDh"
      },
      "source": [
        "# Create Oasis3 data class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXkVpUwV4Xnn"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])# https://github.com/yunjey/pytorch-tutorial/issues/161\n",
        "        \n",
        "    ]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNw2JbDA3UUY"
      },
      "source": [
        "train_image_number = 140 # Image slice from MRI scans to use for training data\n",
        "root_dir = '/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/'\n",
        "base_folder = '/pyt_oasis3'\n",
        "non_ad_jpg_folder = './t1w_non_ad_jpgs/' + str(train_image_number) + '/'\n",
        "ad_jpg_folder = './t1w_ad_jpgs/' + str(train_image_number) + '/'\n",
        "\n",
        "class Oasis3(VisionDataset):\n",
        "    def __init__(self,\n",
        "            root: str,\n",
        "            train: bool = True, # Train or test dataset\n",
        "            transform: Optional[Callable] = None,\n",
        "            target_transform: Optional[Callable] = None) -> None:\n",
        "        super(Oasis3, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "        self.train = train # Train or test set\n",
        "\n",
        "        # Set repeatble random number\n",
        "        \n",
        "        self.non_ad_data: Any = []\n",
        "        self.non_ad_targets = []\n",
        "\n",
        "        self.ad_data: Any = []\n",
        "        self.ad_targets = []\n",
        "        \n",
        "        # load both datasets\n",
        "        for file_path in glob.glob(ad_jpg_folder+'**.jpg'):\n",
        "            with open (file_path, 'rb') as f:\n",
        "                # image needs to be a PIL image\n",
        "                img = Image.open(f)\n",
        "                # Resize all images 176, 256, 3 -> 256, 256, 0\n",
        "                dsize = (256, 256)\n",
        "                resized = img.resize(dsize)\n",
        "                self.ad_data.append(resized)\n",
        "                self.ad_targets.append(1) # based on the jpg_folder in for loop\n",
        "        self.ad_data = np.vstack(self.ad_data).reshape(-1, 256, 256)\n",
        "        #self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        self.ad_data = self.ad_data.transpose((0, 1, 2))  # convert to HWC\n",
        "\n",
        "        for file_path in glob.glob(non_ad_jpg_folder+'**.jpg'):\n",
        "            with open (file_path, 'rb') as f:\n",
        "                # image needs to be a PIL image\n",
        "                img = Image.open(f)\n",
        "                # Resize all images 176, 256, 3 -> 256, 256, 0\n",
        "                dsize = (256, 256)\n",
        "                resized = img.resize(dsize)\n",
        "                self.non_ad_data.append(resized)\n",
        "                self.non_ad_targets.append(0) # based on the jpg_folder in for loop\n",
        "        self.non_ad_data = np.vstack(self.non_ad_data).reshape(-1, 256, 256)\n",
        "        #self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        self.non_ad_data = self.non_ad_data.transpose((0, 1, 2))  # convert to HWC\n",
        "        \n",
        "        # shuffle the datasets and bucket them as test and train\n",
        "        (self.non_ad_data, self.non_ad_targets) = self.shuffle_associated_arrays(self.non_ad_data, self.non_ad_targets)\n",
        "        (self.ad_data, self.ad_targets) = self.shuffle_associated_arrays(self.ad_data, self.ad_targets)\n",
        "\n",
        "        all_data = np.concatenate((self.non_ad_data, self.ad_data), axis=0)\n",
        "        all_targets = self.non_ad_targets + self.ad_targets\n",
        "        # concat the lists and random shuffly\n",
        "        random.Random(1).shuffle(all_data)\n",
        "        random.Random(1).shuffle(all_targets)\n",
        "\n",
        "        # Use scikit learn to split complete data into test and train\n",
        "        self.train_data, self.test_data, self.train_targets, self.test_targets =\\\n",
        "         train_test_split(all_data, all_targets, test_size=0.1, random_state=42)\n",
        "\n",
        "        # Set test or train data based on input\n",
        "        if self.train:\n",
        "          self.data = self.train_data\n",
        "          self.targets = self.train_targets\n",
        "        else:\n",
        "          self.data = self.test_data\n",
        "          self.targets = self.test_targets\n",
        "\n",
        "    def shuffle_associated_arrays(self, data, targets):\n",
        "        c = list(zip(data, targets))\n",
        "        random.shuffle(c)\n",
        "        data, targets = zip(*c)\n",
        "        return (np.array(data), list(targets))\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        \n",
        "        img = Image.fromarray(img)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return (img, target)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t53epg7x3UUY"
      },
      "source": [
        "# Issue is i need to make the data transformable inside __getitem__\n",
        "#a=Oasis3(base_folder, False, transform=transform)\n",
        "\n",
        "dataset = {}\n",
        "dataset['train'] = Oasis3(root='./data', train=True, transform=transform)\n",
        "dataset['test'] = Oasis3(root='./data', train=False, transform=transform)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JzUbocMHkcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971d5531-f18b-4f64-cf50-4f61cf769ef0"
      },
      "source": [
        "# The class should be index into which invokes the __getitem__ call\n",
        "print(dataset['train'][0][0].shape)\n",
        "print(dataset['train'][0][0])\n",
        "print(dataset['train'][0][1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 256])\n",
            "tensor([[[-0.9686, -0.9765, -0.9922,  ..., -0.9843, -0.9843, -1.0000],\n",
            "         [-0.9765, -0.9765, -0.9765,  ..., -0.9843, -0.9843, -1.0000],\n",
            "         [-0.9765, -0.9765, -0.9765,  ..., -0.9843, -0.9843, -1.0000],\n",
            "         ...,\n",
            "         [-0.9922, -0.9686, -0.9608,  ..., -0.9843, -0.9922, -1.0000],\n",
            "         [-0.9922, -0.9686, -0.9608,  ..., -0.9843, -0.9922, -1.0000],\n",
            "         [-0.9843, -0.9843, -0.9608,  ..., -0.9922, -1.0000, -1.0000]]])\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCd0uelFBjN9"
      },
      "source": [
        "def get_oasis_anomaly_dataset(trn_img, trn_lbl, tst_img, tst_lbl, abn_cls_idx=1, manualseed=-1):\r\n",
        "    \"\"\"[summary]\r\n",
        "    Arguments:\r\n",
        "        trn_img {np.array} -- Training images\r\n",
        "        trn_lbl {np.array} -- Training labels\r\n",
        "        tst_img {np.array} -- Test     images\r\n",
        "        tst_lbl {np.array} -- Test     labels\r\n",
        "    Keyword Arguments:\r\n",
        "        abn_cls_idx {int} -- Anomalous class index (default: {1})\r\n",
        "    Returns:\r\n",
        "        [np.array] -- New training-test images and labels.\r\n",
        "    \"\"\"\r\n",
        "    # Convert train-test labels into numpy array.\r\n",
        "    trn_lbl = np.array(trn_lbl)\r\n",
        "    tst_lbl = np.array(tst_lbl)\r\n",
        "\r\n",
        "    # --\r\n",
        "    # Find idx, img, lbl for abnormal and normal on org dataset.\r\n",
        "    nrm_trn_idx = list(np.where(trn_lbl != abn_cls_idx))[0]\r\n",
        "    abn_trn_idx = list(np.where(trn_lbl == abn_cls_idx))[0]\r\n",
        "\r\n",
        "    nrm_trn_img = trn_img[nrm_trn_idx]    # Normal training images\r\n",
        "    abn_trn_img = trn_img[abn_trn_idx]    # Abnormal training images\r\n",
        "    nrm_trn_lbl = trn_lbl[nrm_trn_idx]    # Normal training labels\r\n",
        "    abn_trn_lbl = trn_lbl[abn_trn_idx]    # Abnormal training labels.\r\n",
        "\r\n",
        "    # nrm_trn_img = [trn_img[i] for i in nrm_trn_idx]\r\n",
        "    # abn_trn_img = [trn_img[i] for i in abn_trn_idx]\r\n",
        "    # nrm_trn_lbl = [trn_lbl[i] for i in nrm_trn_idx]\r\n",
        "    # abn_trn_img = [trn_lbl[i] for i in abn_trn_idx]  \r\n",
        "\r\n",
        "    nrm_tst_idx = list(np.where(tst_lbl != abn_cls_idx))[0]\r\n",
        "    abn_tst_idx = list(np.where(tst_lbl == abn_cls_idx))[0]\r\n",
        "\r\n",
        "    nrm_tst_img = tst_img[nrm_tst_idx]    # Normal training images\r\n",
        "    abn_tst_img = tst_img[abn_tst_idx]    # Abnormal training images.\r\n",
        "    nrm_tst_lbl = tst_lbl[nrm_tst_idx]    # Normal training labels\r\n",
        "    abn_tst_lbl = tst_lbl[abn_tst_idx]    # Abnormal training labels.\r\n",
        "\r\n",
        "    # nrm_tst_img = [tst_img[i] for i in nrm_tst_idx]\r\n",
        "    # abn_tst_img = [tst_img[i] for i in abn_tst_idx]\r\n",
        "    # nrm_tst_lbl = [trn_lbl[i] for i in nrm_tst_idx]\r\n",
        "    # abn_tst_lbl = [trn_lbl[i] for i in abn_tst_idx]  \r\n",
        "\r\n",
        "    # --\r\n",
        "    # Assign labels to normal (0) and abnormals (1)\r\n",
        "    nrm_trn_lbl[:] = 0\r\n",
        "    nrm_tst_lbl[:] = 0\r\n",
        "    abn_trn_lbl[:] = 1\r\n",
        "    abn_tst_lbl[:] = 1\r\n",
        "\r\n",
        "    # --\r\n",
        "    if manualseed != -1:\r\n",
        "        # Random seed.\r\n",
        "        # Concatenate the original train and test sets.\r\n",
        "        nrm_img = np.concatenate((nrm_trn_img, nrm_tst_img), axis=0)\r\n",
        "        nrm_lbl = np.concatenate((nrm_trn_lbl, nrm_tst_lbl), axis=0)\r\n",
        "        abn_img = np.concatenate((abn_trn_img, abn_tst_img), axis=0)\r\n",
        "        abn_lbl = np.concatenate((abn_trn_lbl, abn_tst_lbl), axis=0)\r\n",
        "\r\n",
        "        # Split the normal data into the new train and tests.\r\n",
        "        idx = np.arange(len(nrm_lbl))\r\n",
        "        np.random.seed(manualseed)\r\n",
        "        np.random.shuffle(idx)\r\n",
        "\r\n",
        "        nrm_trn_len = int(len(idx) * 0.80)\r\n",
        "        nrm_trn_idx = idx[:nrm_trn_len]\r\n",
        "        nrm_tst_idx = idx[nrm_trn_len:]\r\n",
        "\r\n",
        "        nrm_trn_img = nrm_img[nrm_trn_idx]\r\n",
        "        nrm_trn_lbl = nrm_lbl[nrm_trn_idx]\r\n",
        "        nrm_tst_img = nrm_img[nrm_tst_idx]\r\n",
        "        nrm_tst_lbl = nrm_lbl[nrm_tst_idx]\r\n",
        "\r\n",
        "    # Create new anomaly dataset based on the following data structure:\r\n",
        "    # - anomaly dataset\r\n",
        "    #   . -> train\r\n",
        "    #        . -> normal\r\n",
        "    #   . -> test\r\n",
        "    #        . -> normal\r\n",
        "    #        . -> abnormal\r\n",
        "    new_trn_img = np.copy(nrm_trn_img)\r\n",
        "    new_trn_lbl = np.copy(nrm_trn_lbl)\r\n",
        "    new_tst_img = np.concatenate((nrm_tst_img, abn_trn_img, abn_tst_img), axis=0)\r\n",
        "    new_tst_lbl = np.concatenate((nrm_tst_lbl, abn_trn_lbl, abn_tst_lbl), axis=0)\r\n",
        "\r\n",
        "    return new_trn_img, new_trn_lbl, new_tst_img, new_tst_lbl"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ElgmnwHOm11"
      },
      "source": [
        "dataset['train'].data, dataset['train'].targets, \\\r\n",
        "dataset['test'].data, dataset['test'].targets = get_oasis_anomaly_dataset(\r\n",
        "    trn_img=dataset['train'].data,\r\n",
        "    trn_lbl=dataset['train'].targets,\r\n",
        "    tst_img=dataset['test'].data,\r\n",
        "    tst_lbl=dataset['test'].targets,\r\n",
        "    abn_cls_idx=1\r\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sec3F_JTzQF",
        "outputId": "22f6bd03-f5d7-4de1-81fd-7993954dd84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%env PYTHONPATH=\"$/env/python:/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/ganomaly\"\r\n",
        "!echo $PYTHONPATH"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\"$/env/python:/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/ganomaly\"\n",
            "\"$/env/python:/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/ganomaly\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq_wfQQFTtx3",
        "outputId": "cb0fe6d1-3c9d-4332-f007-b9551ee1caf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\r\n",
        "\r\n",
        "from ganomaly.options import Options\r\n",
        "from ganomaly.lib.data import load_data"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"$/env/python:/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/ganomaly\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PamAClWCTxMr",
        "outputId": "74c217ee-0b95-429a-a732-292583fa938e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "from ganomaly.lib.model import Ganomaly"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f7b980caf7c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mganomaly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGanomaly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/1bqqm9-0FMOqiqyH_NIQBh-lJ36pWa7aX/Masters_Project/Datasets/OASIS3/ganomaly/lib/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4SZOmgxeEfC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}