{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-load_oasis3_data.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvU5vMeB3tL8",
        "outputId": "89015173-74e3-4e4d-ed82-7b1fd97c39df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPD5t7AP35Ba",
        "outputId": "5a55fd9b-ea48-41e9-bcf9-0c9cac1f4dbc"
      },
      "source": [
        "%cd /content/drive/MyDrive/Masters_Project/Datasets/OASIS3/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1bqqm9-0FMOqiqyH_NIQBh-lJ36pWa7aX/Masters_Project/Datasets/OASIS3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZu16iC63UUK",
        "outputId": "c154f7d0-c1c2-4874-9a70-274b1df15e80"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cifar-10-batches-py\t  fa20\t\t    t1w_non_ad_patients\n",
            " cifar-10-python.tar.gz   oasis-scripts    'Usage Instructions.gdoc'\n",
            " data\t\t\t  t1w_ad_jpgs\n",
            " downloaded_data\t  t1w_non_ad_jpgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dXxoGdws3UUV",
        "outputId": "d9c8b96c-94c5-473b-a834-c4f9f5bce0cb"
      },
      "source": [
        "!pip install -r ../../Code/ganomaly/requirements.txt\r\n",
        "#!pip install -r ./ganomaly/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: asn1crypto==0.24.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 1)) (0.24.0)\n",
            "Requirement already satisfied: certifi==2019.6.16 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 2)) (2019.6.16)\n",
            "Requirement already satisfied: cffi==1.12.3 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 3)) (1.12.3)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: cryptography==2.7 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 5)) (2.7)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: idna==2.8 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 7)) (2.8)\n",
            "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 8)) (0.13.2)\n",
            "Requirement already satisfied: kiwisolver==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: matplotlib==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 10)) (3.1.0)\n",
            "Collecting numpy==1.16.4\n",
            "  Using cached https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile==0.46 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 12)) (0.46)\n",
            "Requirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 13)) (8.1.0)\n",
            "Requirement already satisfied: pycparser==2.19 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 14)) (2.19)\n",
            "Requirement already satisfied: pyOpenSSL==19.0.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 15)) (19.0.0)\n",
            "Requirement already satisfied: pyparsing==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 16)) (2.4.0)\n",
            "Requirement already satisfied: PySocks==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 17)) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 18)) (2.8.0)\n",
            "Requirement already satisfied: pytz==2019.1 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 19)) (2019.1)\n",
            "Requirement already satisfied: pyzmq==18.0.2 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 20)) (18.0.2)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 21)) (2.22.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 22)) (0.21.2)\n",
            "Requirement already satisfied: scipy==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 23)) (1.3.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 24)) (1.12.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 25)) (1.2.0)\n",
            "Requirement already satisfied: torchfile==0.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 26)) (0.1.0)\n",
            "Requirement already satisfied: torchvision==0.4 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 27)) (0.4.0)\n",
            "Requirement already satisfied: tornado==6.0.3 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 28)) (6.0.3)\n",
            "Requirement already satisfied: tqdm==4.33.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 29)) (4.33.0)\n",
            "Requirement already satisfied: urllib3==1.25.3 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 30)) (1.25.3)\n",
            "Requirement already satisfied: visdom==0.1.8.8 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 31)) (0.1.8.8)\n",
            "Requirement already satisfied: websocket-client==0.56.0 in /usr/local/lib/python3.7/dist-packages (from -r ../../Code/ganomaly/requirements.txt (line 32)) (0.56.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from kiwisolver==1.1.0->-r ../../Code/ganomaly/requirements.txt (line 9)) (53.0.0)\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.21.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mkl-fft 1.2.0 has requirement numpy==1.19.5, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement joblib>=0.14, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.16.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "LJXouR6y3UUW",
        "outputId": "46d21f4c-26a5-4111-e7f3-498f252e8b10"
      },
      "source": [
        "!pip install mkl-fft"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from mkl-fft) (2019.0)\n",
            "Requirement already satisfied: dpcpp_cpp_rt in /usr/local/lib/python3.7/dist-packages (from mkl-fft) (2021.1.2)\n",
            "Collecting numpy==1.19.5\n",
            "  Using cached https://files.pythonhosted.org/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: common-cmplr-lib-rt==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: common-cmplr-lic-rt==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: opencl-rt==2021.* in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft) (2021.1.2)\n",
            "Requirement already satisfied: tbb==2021.* in /usr/local/lib/python3.7/dist-packages (from opencl-rt==2021.*->dpcpp_cpp_rt->mkl-fft) (2021.1.1)\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.21.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement joblib>=0.14, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.33.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed numpy-1.19.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooHk9Hpm5aww",
        "outputId": "7627055c-6669-4b43-fb2e-6fc30f8776b0"
      },
      "source": [
        "!pip install Pillow"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (8.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWYyHTRj9Pi6"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPv2uxvO3UUW"
      },
      "source": [
        "import os\n",
        "#import os.path\n",
        "#import cv2\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "from typing import Any, Callable, Optional, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.datasets.utils import check_integrity, download_and_extract_archive"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxCZS52C3UUW",
        "outputId": "c9c8a85a-11f7-497e-81ad-0ba9e456bd33"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oJArWMRm3UUX",
        "outputId": "116e1cd8-0d60-4214-c2b7-d2e3c6f07386"
      },
      "source": [
        "import torchvision\n",
        "torchvision.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTPtCs-A9RDh"
      },
      "source": [
        "# Create Oasis3 data class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXkVpUwV4Xnn"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])# https://github.com/yunjey/pytorch-tutorial/issues/161\n",
        "        \n",
        "    ]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNw2JbDA3UUY"
      },
      "source": [
        "train_image_number = 140 # Image slice from MRI scans to use for training data\n",
        "root_dir = '/content/drive/MyDrive/Masters_Project/Datasets/OASIS3/'\n",
        "base_folder = '/pyt_oasis3'\n",
        "non_ad_jpg_folder = './t1w_non_ad_jpgs/' + str(train_image_number) + '/'\n",
        "ad_jpg_folder = './t1w_ad_jpgs/' + str(train_image_number) + '/'\n",
        "\n",
        "class Oasis3(VisionDataset):\n",
        "    def __init__(self,\n",
        "            root: str,\n",
        "            train: bool = True, # Train or test dataset\n",
        "            transform: Optional[Callable] = None,\n",
        "            target_transform: Optional[Callable] = None) -> None:\n",
        "        super(Oasis3, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "        self.train = train # Train or test set\n",
        "\n",
        "        # Set repeatble random number\n",
        "        \n",
        "        self.non_ad_data = []\n",
        "        self.non_ad_targets = []\n",
        "\n",
        "        self.ad_data = []\n",
        "        self.ad_targets = []\n",
        "        \n",
        "        # load both datasets\n",
        "        for file_path in glob.glob(ad_jpg_folder+'**.jpg'):\n",
        "            with open (file_path, 'rb') as f:\n",
        "                # image needs to be a PIL image\n",
        "                img = Image.open(f)\n",
        "                # Resize all images 176, 256, 3 -> 256, 256, 0\n",
        "                dsize = (256, 256)\n",
        "                resized = img.resize(dsize)\n",
        "                self.ad_data.append(resized)\n",
        "                self.ad_targets.append(1) # based on the jpg_folder in for loop\n",
        "        self.ad_data = np.vstack(self.ad_data).reshape(-1, 256, 256)\n",
        "        #self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        self.ad_data = self.ad_data.transpose((0, 1, 2))  # convert to HWC\n",
        "\n",
        "        for file_path in glob.glob(non_ad_jpg_folder+'**.jpg'):\n",
        "            with open (file_path, 'rb') as f:\n",
        "                # image needs to be a PIL image\n",
        "                img = Image.open(f)\n",
        "                # Resize all images 176, 256, 3 -> 256, 256, 0\n",
        "                dsize = (256, 256)\n",
        "                resized = img.resize(dsize)\n",
        "                self.non_ad_data.append(resized)\n",
        "                self.non_ad_targets.append(0) # based on the jpg_folder in for loop\n",
        "        self.non_ad_data = np.vstack(self.non_ad_data).reshape(-1, 256, 256)\n",
        "        #self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        self.non_ad_data = self.non_ad_data.transpose((0, 1, 2))  # convert to HWC\n",
        "        \n",
        "        # shuffle the datasets and bucket them as test and train\n",
        "        (self.non_ad_data, self.non_ad_targets) = self.shuffle_associated_arrays(self.non_ad_data, self.non_ad_targets)\n",
        "        (self.ad_data, self.ad_targets) = self.shuffle_associated_arrays(self.ad_data, self.ad_targets)\n",
        "\n",
        "        all_data = self.non_ad_data + self.ad_data\n",
        "        all_targets = self.non_ad_targets + self.ad_targets\n",
        "        # concat the lists and random shuffly\n",
        "        random.Random(1).shuffle(all_data)\n",
        "        random.Random(1).shuffle(all_targets)\n",
        "\n",
        "        # Use scikit learn to split complete data into test and train\n",
        "        self.train_data, self.test_data, self.train_targets, self.test_targets =\\\n",
        "         train_test_split(all_data, all_targets, test_size=0.1, random_state=42)\n",
        "\n",
        "        # Set test or train data based on input\n",
        "        if self.train:\n",
        "          self.data = self.train_data\n",
        "          self.targets = self.train_targets\n",
        "        else:\n",
        "          self.data = self.test_data\n",
        "          self.targets = self.test_targets\n",
        "\n",
        "    def shuffle_associated_arrays(self, arr1, arr2):\n",
        "        c = list(zip(arr1, arr2))\n",
        "        random.shuffle(c)\n",
        "        arr1, arr2 = zip(*c)\n",
        "        return (list(arr1), list(arr2))\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        \n",
        "        img = Image.fromarray(img)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return (img, target)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t53epg7x3UUY"
      },
      "source": [
        "# Issue is i need to make the data transformable inside __getitem__\n",
        "a=Oasis3_train_normal(base_folder, False, transform=transform)\n",
        "\n",
        "dataset = {}\n",
        "dataset['train'] = Oasis3(root='./data', train=True, transform=transform)\n",
        "dataset['test'] = Oasis3(root='./data', train=False, transform=transform)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JzUbocMHkcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c100bf89-dd07-4d5d-e4e2-ea2251a7ae16"
      },
      "source": [
        "# The class should be index into which invokes the __getitem__ call\n",
        "print(dataset['train'][0][0].shape)\n",
        "print(dataset['train'][0][0])\n",
        "print(dataset['train'][0][1])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 256])\n",
            "tensor([[[-0.9686, -0.9529, -0.9451,  ..., -0.9843, -0.9529, -1.0000],\n",
            "         [-0.9686, -0.9608, -0.9686,  ..., -0.9529, -0.9529, -1.0000],\n",
            "         [-0.9765, -0.9686, -0.9922,  ..., -0.9451, -0.9608, -1.0000],\n",
            "         ...,\n",
            "         [-0.9922, -0.9294, -0.9922,  ..., -0.9686, -0.9843, -0.9922],\n",
            "         [-0.9843, -0.9451, -0.9922,  ..., -0.9765, -0.9922, -0.9922],\n",
            "         [-0.9843, -0.9765, -0.9686,  ..., -0.9765, -0.9922, -1.0000]]])\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCd0uelFBjN9"
      },
      "source": [
        "def get_oasis_anomaly_dataset(trn_img, trn_lbl, tst_img, tst_lbl, abn_cls_idx=1, manualseed=-1):\r\n",
        "    \"\"\"[summary]\r\n",
        "    Arguments:\r\n",
        "        trn_img {np.array} -- Training images\r\n",
        "        trn_lbl {np.array} -- Training labels\r\n",
        "        tst_img {np.array} -- Test     images\r\n",
        "        tst_lbl {np.array} -- Test     labels\r\n",
        "    Keyword Arguments:\r\n",
        "        abn_cls_idx {int} -- Anomalous class index (default: {1})\r\n",
        "    Returns:\r\n",
        "        [np.array] -- New training-test images and labels.\r\n",
        "    \"\"\"\r\n",
        "    # Convert train-test labels into numpy array.\r\n",
        "    trn_lbl = np.array(trn_lbl)\r\n",
        "    tst_lbl = np.array(tst_lbl)\r\n",
        "\r\n",
        "    # --\r\n",
        "    # Find idx, img, lbl for abnormal and normal on org dataset.\r\n",
        "    nrm_trn_idx = list(np.where(trn_lbl != abn_cls_idx))[0]\r\n",
        "    abn_trn_idx = list(np.where(trn_lbl == abn_cls_idx))[0]\r\n",
        "\r\n",
        "    print(nrm_trn_idx)\r\n",
        "\r\n",
        "    # nrm_trn_img = trn_img[nrm_trn_idx]    # Normal training images\r\n",
        "    # abn_trn_img = trn_img[abn_trn_idx]    # Abnormal training images\r\n",
        "    # nrm_trn_lbl = trn_lbl[nrm_trn_idx]    # Normal training labels\r\n",
        "    # abn_trn_lbl = trn_lbl[abn_trn_idx]    # Abnormal training labels.\r\n",
        "\r\n",
        "    nrm_trn_img = [trn_img[i] for i in nrm_trn_idx]\r\n",
        "    abn_trn_img = [trn_img[i] for i in abn_trn_idx]\r\n",
        "    nrm_trn_lbl = [trn_lbl[i] for i in nrm_trn_idx]\r\n",
        "    abn_trn_img = [trn_lbl[i] for i in abn_trn_idx]  \r\n",
        "\r\n",
        "    nrm_tst_idx = list(np.where(tst_lbl != abn_cls_idx))[0]\r\n",
        "    abn_tst_idx = list(np.where(tst_lbl == abn_cls_idx))[0]\r\n",
        "\r\n",
        "    # nrm_tst_img = tst_img[nrm_tst_idx]    # Normal training images\r\n",
        "    # abn_tst_img = tst_img[abn_tst_idx]    # Abnormal training images.\r\n",
        "    # nrm_tst_lbl = tst_lbl[nrm_tst_idx]    # Normal training labels\r\n",
        "    # abn_tst_lbl = tst_lbl[abn_tst_idx]    # Abnormal training labels.\r\n",
        "\r\n",
        "    nrm_tst_img = [tst_img[i] for i in nrm_tst_idx]\r\n",
        "    abn_tst_img = [tst_img[i] for i in abn_tst_idx]\r\n",
        "    nrm_tst_lbl = [trn_lbl[i] for i in nrm_tst_idx]\r\n",
        "    abn_tst_lbl = [trn_lbl[i] for i in abn_tst_idx]  \r\n",
        "\r\n",
        "    # --\r\n",
        "    # Assign labels to normal (0) and abnormals (1)\r\n",
        "    nrm_trn_lbl[:] = 0\r\n",
        "    nrm_tst_lbl[:] = 0\r\n",
        "    abn_trn_lbl[:] = 1\r\n",
        "    abn_tst_lbl[:] = 1\r\n",
        "\r\n",
        "    # --\r\n",
        "    if manualseed != -1:\r\n",
        "        # Random seed.\r\n",
        "        # Concatenate the original train and test sets.\r\n",
        "        nrm_img = np.concatenate((nrm_trn_img, nrm_tst_img), axis=0)\r\n",
        "        nrm_lbl = np.concatenate((nrm_trn_lbl, nrm_tst_lbl), axis=0)\r\n",
        "        abn_img = np.concatenate((abn_trn_img, abn_tst_img), axis=0)\r\n",
        "        abn_lbl = np.concatenate((abn_trn_lbl, abn_tst_lbl), axis=0)\r\n",
        "\r\n",
        "        # Split the normal data into the new train and tests.\r\n",
        "        idx = np.arange(len(nrm_lbl))\r\n",
        "        np.random.seed(manualseed)\r\n",
        "        np.random.shuffle(idx)\r\n",
        "\r\n",
        "        nrm_trn_len = int(len(idx) * 0.80)\r\n",
        "        nrm_trn_idx = idx[:nrm_trn_len]\r\n",
        "        nrm_tst_idx = idx[nrm_trn_len:]\r\n",
        "\r\n",
        "        nrm_trn_img = nrm_img[nrm_trn_idx]\r\n",
        "        nrm_trn_lbl = nrm_lbl[nrm_trn_idx]\r\n",
        "        nrm_tst_img = nrm_img[nrm_tst_idx]\r\n",
        "        nrm_tst_lbl = nrm_lbl[nrm_tst_idx]\r\n",
        "\r\n",
        "    # Create new anomaly dataset based on the following data structure:\r\n",
        "    # - anomaly dataset\r\n",
        "    #   . -> train\r\n",
        "    #        . -> normal\r\n",
        "    #   . -> test\r\n",
        "    #        . -> normal\r\n",
        "    #        . -> abnormal\r\n",
        "    new_trn_img = np.copy(nrm_trn_img)\r\n",
        "    new_trn_lbl = np.copy(nrm_trn_lbl)\r\n",
        "    new_tst_img = np.concatenate((nrm_tst_img, abn_trn_img, abn_tst_img), axis=0)\r\n",
        "    new_tst_lbl = np.concatenate((nrm_tst_lbl, abn_trn_lbl, abn_tst_lbl), axis=0)\r\n",
        "\r\n",
        "    return new_trn_img, new_trn_lbl, new_tst_img, new_tst_lbl"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ElgmnwHOm11",
        "outputId": "944ab4aa-1cd1-4757-f9b0-96f38de1b542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dataset['train'].data, dataset['train'].targets, \\\r\n",
        "dataset['test'].data, dataset['test'].targets = get_oasis_anomaly_dataset(\r\n",
        "    trn_img=dataset['train'].data,\r\n",
        "    trn_lbl=dataset['train'].targets,\r\n",
        "    tst_img=dataset['test'].data,\r\n",
        "    tst_lbl=dataset['test'].targets,\r\n",
        "    abn_cls_idx=1\r\n",
        ")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    2    3    4    5    8   10   11   14   16   17   18   19   20\n",
            "   21   22   23   24   25   26   28   29   31   33   35   36   38   40\n",
            "   41   42   43   44   45   46   48   49   51   52   53   55   56   57\n",
            "   58   59   60   61   62   63   64   66   67   68   69   70   71   72\n",
            "   73   74   75   76   77   78   79   82   83   85   86   87   88   89\n",
            "   90   93   94   95   97   98  101  102  103  105  108  109  110  111\n",
            "  112  113  114  115  116  117  118  119  122  123  125  127  128  130\n",
            "  133  135  140  141  142  143  144  145  146  147  148  149  150  151\n",
            "  153  155  156  157  158  160  161  162  164  165  167  168  169  170\n",
            "  171  172  173  174  175  177  178  179  180  181  184  186  187  189\n",
            "  192  193  194  195  196  197  198  199  200  202  203  204  206  208\n",
            "  209  211  212  213  214  215  218  220  221  222  223  224  225  226\n",
            "  227  229  230  232  233  234  235  236  238  240  242  243  244  245\n",
            "  246  247  248  250  252  253  255  256  257  258  259  260  261  262\n",
            "  263  264  266  267  268  270  271  273  274  276  277  278  279  280\n",
            "  281  282  283  286  287  288  289  290  291  292  293  294  295  298\n",
            "  300  301  302  303  304  305  306  307  308  309  312  313  314  315\n",
            "  316  319  320  321  322  323  324  325  327  328  329  330  335  337\n",
            "  338  339  340  341  344  345  346  349  350  351  352  354  355  356\n",
            "  357  359  361  362  363  365  366  368  369  370  371  372  373  374\n",
            "  375  377  378  379  381  382  383  384  385  386  387  389  390  391\n",
            "  392  393  394  396  397  399  401  402  403  409  413  415  416  418\n",
            "  419  420  422  423  424  425  426  427  429  430  431  434  435  436\n",
            "  437  438  439  440  441  442  445  447  449  450  453  454  455  456\n",
            "  457  458  460  461  462  464  467  469  470  471  472  473  476  477\n",
            "  478  479  480  481  482  484  485  486  487  490  491  492  493  496\n",
            "  497  498  499  501  502  503  504  507  508  509  510  511  515  518\n",
            "  519  520  521  522  523  524  526  528  529  530  532  534  536  537\n",
            "  538  541  542  544  546  550  551  552  554  555  556  558  559  560\n",
            "  561  563  564  565  567  568  569  571  574  576  579  580  581  584\n",
            "  585  587  588  589  590  592  593  594  595  597  599  600  601  602\n",
            "  604  606  607  608  609  611  612  613  614  615  616  617  618  619\n",
            "  620  621  622  623  624  625  626  628  630  633  634  635  638  639\n",
            "  641  642  644  645  647  648  649  650  651  652  653  655  656  657\n",
            "  658  661  662  663  664  665  666  667  670  671  672  673  677  679\n",
            "  681  682  683  685  686  687  688  689  690  691  692  696  697  698\n",
            "  699  700  702  707  709  710  711  712  713  714  715  716  717  718\n",
            "  719  720  721  725  726  727  729  731  732  734  735  736  737  738\n",
            "  740  742  744  746  748  750  751  752  754  756  757  758  760  761\n",
            "  762  763  767  769  770  771  772  773  775  776  777  780  781  782\n",
            "  783  786  787  789  790  793  794  796  797  799  800  802  803  804\n",
            "  805  808  809  810  811  812  814  815  816  817  819  820  821  823\n",
            "  826  828  830  831  833  834  836  837  838  842  843  844  846  847\n",
            "  848  852  853  856  857  858  859  860  861  862  864  865  867  868\n",
            "  869  870  871  873  874  875  876  877  879  880  881  882  884  886\n",
            "  887  889  890  891  892  893  894  895  896  897  898  899  901  902\n",
            "  903  906  908  910  911  912  913  915  916  917  918  919  920  922\n",
            "  923  924  925  926  927  928  929  930  931  932  933  934  936  937\n",
            "  938  939  940  941  942  943  944  945  946  947  948  949  950  951\n",
            "  952  953  954  955  956  957  958  960  961  962  963  964  965  966\n",
            "  972  973  974  976  977  978  979  980  981  983  984  985  987  988\n",
            "  989  990  992  993  994  995  996  998  999 1000 1001 1002 1003 1004\n",
            " 1005 1007 1009 1010 1011 1012 1014 1015 1016 1017 1018 1019 1020 1021\n",
            " 1023 1024 1025 1026 1027 1029 1030 1031 1032 1034 1035 1036 1037 1038\n",
            " 1039 1040 1041 1042 1043 1044 1047 1048 1049 1050 1051 1052 1053 1054\n",
            " 1055 1056 1058 1060 1061 1062 1063 1064 1066 1067 1070 1071 1072 1073\n",
            " 1074 1075 1077 1079 1080 1082 1083 1086 1087 1089 1090 1091 1092 1093\n",
            " 1094 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108\n",
            " 1110 1113 1115 1116 1117 1118 1120 1121 1122 1123 1124 1126 1127 1128\n",
            " 1129 1132 1134 1135 1136 1138 1139 1140 1142 1144 1145 1146 1148 1149\n",
            " 1150 1151 1152 1153 1155 1156 1157 1158 1159 1160 1162 1163 1164]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-63f05ec1bb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtst_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtst_lbl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mabn_cls_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-61-654484e284f3>\u001b[0m in \u001b[0;36mget_oasis_anomaly_dataset\u001b[0;34m(trn_img, trn_lbl, tst_img, tst_lbl, abn_cls_idx, manualseed)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# --\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Assign labels to normal (0) and abnormals (1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mnrm_trn_lbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mnrm_tst_lbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mabn_trn_lbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only assign an iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kQJ7RzjT1G8"
      },
      "source": [
        "train_label = dataset['train'].targets"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMcIu69a1GO4"
      },
      "source": [
        "train_label = np.array(train_label)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlbbEbQN1G14",
        "outputId": "6eafb489-45a0-442c-c58e-7f3f74db7757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trn_lbl_no_ad = np.where(train_label != 1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSEk0anY6nbE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}